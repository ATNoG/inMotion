\section{Results}\label{sec:results}

This section presents the experimental results obtained from training and evaluating a comprehensive set of machine learning classifiers on the RSSI-based passenger movement dataset across three experimental scenarios: combined, isolated-only, and noisy-only conditions. The evaluation encompasses 38 classification algorithms assessed across multiple random seeds to ensure statistical robustness.

\subsection{Classification Performance: Combined Dataset}

\autoref{tab:top_classifiers} summarizes the performance of the top-10 classifiers on the combined dataset, ranked by mean MCC across the experimental seeds.

\begin{table}[!ht]
    \centering
    \caption{Top-10 Classifiers on Combined Dataset (Ranked by MCC)}
    \label{tab:top_classifiers}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Classifier}             & \textbf{Accuracy} & \textbf{Recall}   & \textbf{F1-Score} & \textbf{MCC}      \\
            \midrule
            GaussianProcess                 & 0.816 $\pm$ 0.024 & 0.816 $\pm$ 0.024 & 0.815 $\pm$ 0.024 & 0.756 $\pm$ 0.033 \\
            SVC (RBF)                       & 0.815 $\pm$ 0.015 & 0.815 $\pm$ 0.015 & 0.813 $\pm$ 0.014 & 0.755 $\pm$ 0.021 \\
            SVC (Linear)                    & 0.813 $\pm$ 0.017 & 0.813 $\pm$ 0.017 & 0.812 $\pm$ 0.016 & 0.750 $\pm$ 0.023 \\
            StackingEnsemble                & 0.811 $\pm$ 0.015 & 0.811 $\pm$ 0.015 & 0.810 $\pm$ 0.014 & 0.749 $\pm$ 0.020 \\
            CatBoost                        & 0.809 $\pm$ 0.013 & 0.809 $\pm$ 0.013 & 0.809 $\pm$ 0.013 & 0.746 $\pm$ 0.017 \\
            RandomForest                    & 0.808 $\pm$ 0.017 & 0.808 $\pm$ 0.017 & 0.807 $\pm$ 0.016 & 0.744 $\pm$ 0.022 \\
            LogisticRegression (L1)         & 0.808 $\pm$ 0.031 & 0.808 $\pm$ 0.031 & 0.806 $\pm$ 0.031 & 0.744 $\pm$ 0.042 \\
            LogisticRegression (ElasticNet) & 0.808 $\pm$ 0.031 & 0.808 $\pm$ 0.031 & 0.806 $\pm$ 0.031 & 0.744 $\pm$ 0.042 \\
            MLP (Large)                     & 0.806 $\pm$ 0.015 & 0.806 $\pm$ 0.015 & 0.805 $\pm$ 0.014 & 0.743 $\pm$ 0.021 \\
            LogisticRegression (L2)         & 0.806 $\pm$ 0.033 & 0.806 $\pm$ 0.033 & 0.805 $\pm$ 0.033 & 0.743 $\pm$ 0.044 \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

The Gaussian Process classifier achieved the highest mean MCC (0.756) with accuracy of 81.6\%, demonstrating strong discriminative capability for the temporal RSSI patterns. Support Vector Machines with RBF and linear kernels followed closely with MCC values of 0.755 and 0.750, respectively.

\subsection{Comparative Analysis Across Experimental Scenarios}

\autoref{tab:scenario_comparison} presents the performance comparison of top classifiers across the three experimental scenarios, revealing the substantial impact of data collection conditions on classification performance.

\begin{table}[!ht]
    \centering
    \caption{Performance Comparison Across Experimental Scenarios (MCC)}
    \label{tab:scenario_comparison}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lccc}
            \toprule
            \textbf{Classifier}     & \textbf{Combined}     & \textbf{Isolated-Only}    & \textbf{Noisy-Only}       \\
            \midrule
            KNN (k=5)               & 0.692 $\pm$ 0.029     & \textbf{0.907 $\pm$ 0.061} & 0.704 $\pm$ 0.025         \\
            KNN (k=3)               & 0.690 $\pm$ 0.046     & 0.882 $\pm$ 0.068          & 0.702 $\pm$ 0.043         \\
            LinearSVC               & 0.726 $\pm$ 0.039     & 0.867 $\pm$ 0.060          & 0.716 $\pm$ 0.031         \\
            LogisticRegression (L2) & 0.743 $\pm$ 0.044     & 0.866 $\pm$ 0.028          & 0.731 $\pm$ 0.009         \\
            SVC (Linear)            & 0.750 $\pm$ 0.023     & 0.850 $\pm$ 0.088          & 0.731 $\pm$ 0.037         \\
            StackingEnsemble        & 0.749 $\pm$ 0.020     & 0.851 $\pm$ 0.122          & 0.768 $\pm$ 0.023         \\
            ExtraTrees              & 0.737 $\pm$ 0.013     & 0.836 $\pm$ 0.041          & 0.755 $\pm$ 0.021         \\
            GaussianProcess         & \textbf{0.756 $\pm$ 0.033} & 0.414 $\pm$ 0.052     & 0.755 $\pm$ 0.028         \\
            SVC (RBF)               & 0.755 $\pm$ 0.021     & 0.825 $\pm$ 0.101          & 0.754 $\pm$ 0.016         \\
            CatBoost                & 0.746 $\pm$ 0.017     & 0.782 $\pm$ 0.063          & \textbf{0.770 $\pm$ 0.013} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

The isolated-only scenario yielded the highest classification performance, with KNN (k=5) achieving an MCC of 0.907, representing a 20\% improvement over the combined dataset. This substantial increase is attributable to the absence of inter-device signal interference during data collection, resulting in cleaner RSSI patterns with more distinct class separations. Notably, simpler classifiers such as KNN exhibited the most pronounced performance gains in the isolated scenario, suggesting that the underlying class boundaries are well-defined when noise is absent.

Conversely, the noisy-only scenario demonstrated performance levels comparable to the combined dataset, with CatBoost achieving the highest MCC of 0.770. The Gaussian Process classifier maintained robust performance (MCC: 0.755) across both combined and noisy-only conditions, but exhibited severe degradation in the isolated scenario (MCC: 0.414), likely due to overfitting on the limited sample size ($n = 159$).

\subsection{Per-Class Analysis}

\autoref{tab:class_performance} presents the per-class accuracy, recall, F1-score, and MCC for the best classifier (Gaussian Process, seed~3).

\begin{table}[!ht]
    \centering
    \caption{Per-Class Performance Metrics (Gaussian Process)}
    \label{tab:class_performance}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Class}        & \textbf{Accuracy} & \textbf{Recall} & \textbf{F1-Score} & \textbf{MCC}   \\
            \midrule
            AA (Inside)           & 0.838             & 0.750           & 0.791             & 0.785          \\
            BB (Stop)             & 0.838             & 0.897           & 0.878             & 0.785          \\
            BA (Boarding)         & 0.838             & 0.824           & 0.855             & 0.785          \\
            AB (Alighting)        & 0.838             & 0.882           & 0.828             & 0.785          \\
            \midrule
            \textbf{Weighted Avg} & \textbf{0.838}    & \textbf{0.838}  & \textbf{0.838}    & \textbf{0.785} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

The static state at the bus stop (BB) exhibited the highest recall (89.7\%) and strong F1-score (0.878), attributable to the consistent low RSSI values observed when devices remain outside the vehicle. The boarding movement (BA) also achieved strong results (recall: 82.4\%, F1-score: 0.855, MCC: 0.785), benefiting from the distinctive increasing RSSI pattern as devices approach the access point. The alighting class (AB) demonstrated high recall (88.2\%) but lower F1-score (0.828), indicating some false positives from the AA class. Conversely, the static state inside the vehicle (AA) presented the most challenging classification scenario, with the lowest recall (75.0\%) and F1-score (0.791), primarily due to confusion with the boarding class (BA), as both involve proximity to the access point. The overall accuracy of 83.8\% and MCC of 0.785 confirm robust multi-class discrimination across all movement categories.

\subsection{Hyperparameter Configuration}

The Gaussian Process classifier employed a Radial Basis Function (RBF) kernel, which is well-suited for capturing non-linear relationships in the RSSI feature space. The model configuration used is presented in \autoref{tab:hyperparams}.

\begin{table}[!ht]
    \centering
    \caption{Gaussian Process Hyperparameters}
    \label{tab:hyperparams}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}   & \textbf{Value}               \\
        \midrule
        Kernel               & $1.0 \times \text{RBF}(1.0)$ \\
        Kernel Length Scale  & Optimized during fitting     \\
        Optimizer            & L-BFGS-B                     \\
        Max Iterations       & 100                          \\
        Multi-class Strategy & One-vs-Rest                  \\
        \bottomrule
    \end{tabular}
\end{table}

The RBF kernel automatically learns the optimal length scale parameter during training, adapting to the intrinsic dimensionality of the RSSI temporal sequences. This flexibility enables the Gaussian Process to model complex decision boundaries without requiring extensive manual hyperparameter tuning, making it particularly suitable for RSSI-based classification where signal patterns exhibit non-linear spatial dependencies.

\subsection{Confusion Matrix Analysis}

\autoref{fig:confusion_matrix} presents the normalized confusion matrix for the Gaussian Process classifier, which achieved the best average performance across experimental seeds.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{images/confusion_matrix_GaussianProcess.pdf}
    \caption{Confusion matrix for the Gaussian Process classifier (seed~3), demonstrating strong diagonal dominance with minimal inter-class confusion.}
    \label{fig:confusion_matrix}
\end{figure}

The confusion matrix reveals that the primary source of classification errors occurs between spatially adjacent classes. Specifically, the AA class (remaining inside) is occasionally misclassified as BA (boarding), reflecting the similarity in RSSI magnitude when devices are positioned near the access point. Similarly, minor confusion exists between AB (alighting) and BB (remaining at stop), as both classes share lower RSSI values characteristic of the exterior zone.

\subsection{Model Stability Analysis}

To evaluate the robustness of classifier rankings across different experimental conditions, \autoref{fig:accuracy_variability} illustrates the accuracy variability for the top classifiers across the three random seeds.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{images/accuracy_variability.pdf}
    \caption{Accuracy variability across experimental seeds for top classifiers, demonstrating consistent ranking stability.}
    \label{fig:accuracy_variability}
\end{figure}

The analysis confirms that top-performing classifiers maintain consistent relative rankings across seeds, with kernel-based methods (Gaussian Process, SVC) and ensemble approaches (Stacking, CatBoost) exhibiting the lowest variability. This stability is crucial for deployment scenarios where model retraining may occur with different data partitions.

\subsection{Feature Importance}

Analysis of feature importance across interpretable classifiers revealed that the initial RSSI measurements (features 1--3) contribute most significantly to classification decisions, as illustrated in \autoref{fig:feature_importance}. This finding aligns with the temporal dynamics of passenger movements, where early signal readings capture the initial position before any state transition occurs.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{images/mean_feature_importance.pdf}
    \caption{Mean feature importance across classifiers, highlighting the discriminative value of initial RSSI measurements.}
    \label{fig:feature_importance}
\end{figure}

The first RSSI sample exhibits the highest importance (normalized score: 2.29), followed by samples at positions 6 and 2. This pattern suggests that classifiers leverage both the starting signal strength and mid-trajectory measurements to infer movement direction, while later samples provide confirmatory information about the final position.