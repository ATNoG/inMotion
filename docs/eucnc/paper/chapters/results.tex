\section{Results}\label{sec:results}

This section presents the experimental results from training and evaluating an extensive set of machine learning classifiers on the \ac{RSSI}-based passenger movement dataset across three experimental scenarios: combined, isolated-only, and noisy-only conditions. The evaluation covers 38 classification algorithms assessed across multiple random seeds to ensure statistical robustness.

\subsection{Comparative Analysis Across Experimental Scenarios}

\autoref{tab:scenario_comparison} presents the performance comparison of top classifiers across the three experimental scenarios, revealing the substantial impact of data collection conditions on classification performance.

\begin{table}[!ht]
    \centering
    \caption{Performance Comparison Across Experimental Scenarios (\ac{MCC})}
    \label{tab:scenario_comparison}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lccc}
            \toprule
            \textbf{Classifier}     & \textbf{Combined}          & \textbf{Isolated-Only}     & \textbf{Noisy-Only}        \\
            \midrule
            KNN (k=5)               & 0.692 $\pm$ 0.029          & \textbf{0.907 $\pm$ 0.061} & 0.704 $\pm$ 0.025          \\
            KNN (k=3)               & 0.690 $\pm$ 0.046          & 0.882 $\pm$ 0.068          & 0.702 $\pm$ 0.043          \\
            LinearSVC               & 0.726 $\pm$ 0.039          & 0.867 $\pm$ 0.060          & 0.716 $\pm$ 0.031          \\
            LogisticRegression (L2) & 0.743 $\pm$ 0.044          & 0.866 $\pm$ 0.028          & 0.731 $\pm$ 0.009          \\
            SVC (Linear)            & 0.750 $\pm$ 0.023          & 0.850 $\pm$ 0.088          & 0.731 $\pm$ 0.037          \\
            StackingEnsemble        & 0.749 $\pm$ 0.020          & 0.851 $\pm$ 0.122          & 0.768 $\pm$ 0.023          \\
            ExtraTrees              & 0.737 $\pm$ 0.013          & 0.836 $\pm$ 0.041          & 0.755 $\pm$ 0.021          \\
            GaussianProcess         & \textbf{0.756 $\pm$ 0.033} & 0.414 $\pm$ 0.052          & 0.755 $\pm$ 0.028          \\
            SVC (RBF)               & 0.755 $\pm$ 0.021          & 0.825 $\pm$ 0.101          & 0.754 $\pm$ 0.016          \\
            CatBoost                & 0.746 $\pm$ 0.017          & 0.782 $\pm$ 0.063          & \textbf{0.770 $\pm$ 0.013} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

The isolated-only scenario yielded the highest classification performance, with \ac{KNN} (k=5) reaching an \ac{MCC} of 0.907---a 20\% improvement over the combined dataset. Simpler classifiers such as \ac{KNN} showed the most pronounced performance gains in the isolated scenario.

The noisy-only scenario exhibited performance levels comparable to the combined dataset, with CatBoost attaining the highest \ac{MCC} of 0.770. The \ac{GP} classifier maintained robust performance (\ac{MCC}: 0.755) across both combined and noisy-only conditions, yet showed notable degradation in the isolated scenario (\ac{MCC}: 0.414).

\subsection{Per-Class Analysis}

\autoref{tab:class_performance} presents the per-class accuracy, recall, F1-score, and \ac{MCC} for the best classifier (\ac{GP}).

\begin{table}[!ht]
    \centering
    \caption{Per-Class Performance Metrics (\ac{GP})}
    \label{tab:class_performance}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Class}        & \textbf{Accuracy} & \textbf{Recall} & \textbf{F1-Score} & \textbf{MCC}   \\
            \midrule
            AA (Inside)           & 0.838             & 0.750           & 0.791             & 0.785          \\
            BB (Stop)             & 0.838             & 0.897           & 0.878             & 0.785          \\
            BA (Boarding)         & 0.838             & 0.824           & 0.855             & 0.785          \\
            AB (Alighting)        & 0.838             & 0.882           & 0.828             & 0.785          \\
            \midrule
            \textbf{Weighted Avg} & \textbf{0.838}    & \textbf{0.838}  & \textbf{0.838}    & \textbf{0.785} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

The static state at the bus stop (BB) exhibited the highest recall (89.7\%) and F1-score (0.878). The boarding movement (BA) achieved recall of 82.4\%, F1-score of 0.855, and \ac{MCC} of 0.785. The alighting class (AB) demonstrated high recall (88.2\%) but lower F1-score (0.828). The static state inside the vehicle (AA) presented the lowest recall (75.0\%) and F1-score (0.791). The overall accuracy reached 83.8\% with an \ac{MCC} of 0.785.

\subsection{Hyperparameter Configuration}

While extensive hyperparameter optimization was conducted for nine classifier families using Optuna (detailed in \autoref{sec:appendix}), the \ac{GP} classifier employed default configurations with a \ac{RBF} kernel, which proved highly effective without explicit tuning. The \ac{GP} configuration is presented in \autoref{tab:hyperparams}.

\begin{table}[!ht]
    \centering
    \caption{\ac{GP} Hyperparameters}
    \label{tab:hyperparams}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}   & \textbf{Value}               \\
        \midrule
        Kernel               & $1.0 \times \text{RBF}(1.0)$ \\
        Kernel Length Scale  & Optimized during fitting     \\
        Optimizer            & L-BFGS-B                     \\
        Max Iterations       & 100                          \\
        Multi-class Strategy & One-vs-Rest                  \\
        \bottomrule
    \end{tabular}
\end{table}

The \ac{RBF} kernel automatically learns the optimal length scale parameter during training, adapting to the intrinsic dimensionality of the \ac{RSSI} temporal sequences. Notably, the \ac{GP} classifier achieved top performance without requiring the extensive hyperparameter search applied to other classifiers, suggesting that its probabilistic formulation is particularly well-suited to the \ac{RSSI} feature space.

\subsection{Confusion Matrix Analysis}

\autoref{fig:confusion_matrix} presents the normalized confusion matrix for the \ac{GP} classifier, which achieved the best average performance across experimental seeds.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{images/confusion_matrix_GaussianProcess.pdf}
    \caption{Confusion matrix for the \ac{GP} classifier, demonstrating strong diagonal dominance with minimal inter-class confusion.}
    \label{fig:confusion_matrix}
\end{figure}

The confusion matrix reveals that the primary source of classification errors occurs between spatially adjacent classes. The AA class (remaining inside) is occasionally misclassified as BA (boarding), and minor confusion exists between AB (alighting) and BB (remaining at stop).

\subsection{Model Stability Analysis}

To evaluate the robustness of classifier rankings across different experimental conditions, \autoref{fig:accuracy_variability} illustrates the accuracy variability for the top classifiers across the three random seeds.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{images/mcc_variability.pdf}
    \caption{\ac{MCC} variability for top classifiers, in combined dataset, demonstrating consistent ranking stability.}
    \label{fig:accuracy_variability}
\end{figure}

The analysis confirms that top-performing classifiers maintain consistent relative rankings across seeds, with kernel-based methods (\ac{GP}, SVC) and ensemble approaches (Stacking, CatBoost) displaying the lowest variability.

\subsection{Feature Importance}

Analysis of feature importance across interpretable classifiers revealed that the initial \ac{RSSI} measurements (features 1--3) contribute most significantly to classification decisions, as illustrated in \autoref{fig:feature_importance}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{images/mean_feature_importance.pdf}
    \caption{Mean feature importance across classifiers, with standard deviation, to the $[0, 1]$ range using min-max normalization}
    \label{fig:feature_importance}
\end{figure}

The first \ac{RSSI} sample exhibits the highest importance (normalized score: 2.29), followed by samples at positions 6 and 2.